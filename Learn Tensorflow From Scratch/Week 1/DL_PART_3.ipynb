{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdbba11",
   "metadata": {},
   "source": [
    "Now let's try to use a pictorial dataset and try to predict what is in it. This will be a very basic computer vision problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a33105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c03cfa",
   "metadata": {},
   "source": [
    "We will use Fashion MNIST dataset which is a collection of grayscale images of 28x28 pixels and are of clothing images. Each image is associated with a label and there are 10 labels in total. That is the target has 10 different classes ranging from 0 to 9. This dataset is available in the tf.keras.datasets API so can make use of this API to call and download the data and use it. There are multiple datasets to use as you will see from here - https://www.tensorflow.org/api_docs/python/tf/keras/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6373d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.api.datasets.fashion_mnist' from 'E:\\\\7. Deep Learning\\\\venv\\\\lib\\\\site-packages\\\\keras\\\\api\\\\datasets\\\\fashion_mnist\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "fmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80155d9a",
   "metadata": {},
   "source": [
    "The output shows you the fashion mnist dataset module getting called from the location as given. Now you need to use load_data() method which will give two tuples with two lists each. The tuples are the training and the testing values. Each tuple has clothing items images and their labels. Let's see -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6efc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (testing_images, testing_labels) = fmnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d842095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images -  (60000, 28, 28)\n",
      "Shape of training labels -  (60000,)\n",
      "Shape of testing images -  (10000, 28, 28)\n",
      "Shape of testing labels -  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape\n",
    "print('Shape of training images - ', training_images.shape)\n",
    "print('Shape of training labels - ', training_labels.shape)\n",
    "print('Shape of testing images - ', testing_images.shape)\n",
    "print('Shape of testing labels - ', testing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50aa58",
   "metadata": {},
   "source": [
    "So there are 60,000 training images and each image is of 28 by 28 pixels. So its a 3D dataset that has the first dimension as the number of records present and the other two dimensions are defining the picture height and width. Similarly there are 10,000 testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d51cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of the Image -   4\n",
      "Pixels of the Image in Array format -  \n",
      " [[  0   0   0   0   0   0   0   5   1   0   0 136 216 204 212 176   0   0   0   2   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   5   1   0   0   0  74 250 249 245 248 235 115  10   0   0   0   4   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0  26 135 226 244 202 237 252 247 206 211 242 208 113   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   3   0  28 158 211 208 192 183 191 211 235 217 187 185 188 199 212 198 109   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 197 215 182 180 178 181 180 182 162 174 182 185 187 184 181 189 220  73   0   5   0   0   0]\n",
      " [  0   0   0   0  94 218 178 185 185 188 182 186 183 216 192 186 188 189 192 183 179 191 165   0   0   0   0   0]\n",
      " [  0   0   0   0 167 205 184 184 197 190 184 189 182 199 189 183 180 183 191 193 187 185 198   9   0   0   0   0]\n",
      " [  0   0   0   0 192 204 185 205 201 187 182 183 181 192 188 186 180 180 195 187 189 187 208  55   0   0   0   0]\n",
      " [  0   0   0   0 215 201 192 232 186 183 185 190 188 193 189 192 182 183 184 233 199 191 210 101   0   0   0   0]\n",
      " [  0   0   0   0 224 197 198 250 193 189 187 198 191 194 193 197 191 183 194 253 214 201 211 183   0   0   0   0]\n",
      " [  0   0   0   0 235 195 206 251 198 194 192 194 192 194 190 197 197 189 197 245 202 217 208 215   0   0   0   0]\n",
      " [  0   0   0  11 243 197 204 250 187 195 194 193 193 197 192 196 197 192 194 231 212 218 207 217   0   0   0   0]\n",
      " [  0   0   0  33 248 195 209 245 190 200 194 192 193 199 193 197 196 190 195 234 225 214 204 235   0   0   0   0]\n",
      " [  0   0   0  54 246 200 212 244 200 201 194 192 195 199 197 191 201 187 203 227 230 201 202 239   6   0   0   0]\n",
      " [  0   0   0  66 245 201 206 246 208 195 192 201 199 199 199 190 196 195 205 220 246 204 198 246  27   0   0   0]\n",
      " [  0   0   0  73 245 194 214 240 192 189 188 204 194 203 199 200 194 191 198 212 254 214 191 246  56   0   0   0]\n",
      " [  0   0   0  81 243 193 213 240 192 198 197 203 194 205 191 200 199 188 206 206 248 229 193 243  74   0   0   0]\n",
      " [  0   0   0  97 238 193 224 233 195 197 201 191 194 204 196 198 201 194 203 184 180 246 197 236  87   0   0   0]\n",
      " [  0   0   0 109 236 197 230 227 192 191 191 188 197 203 193 198 199 201 197 206 201 251 196 230  91   0   0   0]\n",
      " [  0   0   0 118 227 207 235 220 188 189 184 197 201 199 196 199 190 193 190 210 191 254 190 225  99   0   0   0]\n",
      " [  0   0   0 108 223 210 231 186 197 182 189 196 201 194 196 200 188 189 191 221 122 245 195 227  99   0   0   0]\n",
      " [  0   0   0 104 223 212 239 115 214 176 194 206 210 199 200 203 198 192 184 230  42 254 194 211 116   0   0   0]\n",
      " [  0   0   0 110 220 213 252   6 224 200 200 195 190 194 193 192 191 195 196 222   0 249 200 202 125   0   0   0]\n",
      " [  0   0   0 111 202 216 246  32 255 210 207 202 206 210 209 206 202 204 207 255  31 199 213 193 103   0   0   0]\n",
      " [  0   0   0 124 193 216 251   0  58  51  45  57  60  65  63  55  50  42  49  58   0 220 209 199  97   0   0   0]\n",
      " [  0   0   0 106 207 225 176   0   0   0   0   0   0   0   0   0   0   0   0   0   0 148 222 205  83   0   0   0]\n",
      " [  0   0   0  62 218 239 115   0   8   2   6   7   7   7   7   6   5   6   5   5   0  85 234 217  56   0   0   0]\n",
      " [  0   0   0  21 169 192  74   0   1   0   0   0   0   0   0   0   0   0   0   2   0  52 189 184  23   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check how the image looks like\n",
    "\n",
    "random_number = np.random.randint(0, 59999) # It will generate a single number between the given range\n",
    "np.set_printoptions(linewidth=200) # Specifying the number of characters per row while printing\n",
    "\n",
    "print('Label of the Image -  ', training_labels[random_number])  #Looking at the label from the random number generated\n",
    "print('Pixels of the Image in Array format -  \\n', training_images[random_number])\n",
    "\n",
    "plt.imshow(training_images[random_number])  # Looking into the image\n",
    "plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14856c6",
   "metadata": {},
   "source": [
    "It looks like a tshirt. Notice that the numbers of pixels in the array is in between 0 and 255. You need to scale the values between 0 and 1 as that will make the model learn better. This process is called normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610d3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images/255.0\n",
    "testing_images = testing_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e4f945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da19ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to have equal representation of the data in training and testing set. Let's check\n",
    "\n",
    "label_frequency_in_training = dict(Counter(training_labels))\n",
    "label_frequency_in_testing = dict(Counter(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5224568b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: 1000,\n",
       " 2: 1000,\n",
       " 1: 1000,\n",
       " 6: 1000,\n",
       " 4: 1000,\n",
       " 5: 1000,\n",
       " 7: 1000,\n",
       " 3: 1000,\n",
       " 8: 1000,\n",
       " 0: 1000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_frequency_in_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c73255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: 6000,\n",
       " 0: 6000,\n",
       " 3: 6000,\n",
       " 2: 6000,\n",
       " 7: 6000,\n",
       " 5: 6000,\n",
       " 1: 6000,\n",
       " 6: 6000,\n",
       " 4: 6000,\n",
       " 8: 6000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_frequency_in_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8371f",
   "metadata": {},
   "source": [
    "So we can see that testing data has every class present in the training set. There is an equal representation of the data. Now we have seen the dataset and the dataset is normalized as well. We are ready to use it. We will define the model first -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233d5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape=(28,28,1)),  # Since shape of every image is 28 by 28 pixel and 1 stands for the color which is gray here\n",
    "            tf.keras.layers.Flatten(), # We need to use this layer to convert it into 1 dimensional array \n",
    "            tf.keras.layers.Dense(units=128, activation='relu'),  # Using 1 layer of Dense layer with 128 neurons and relu activation\n",
    "            tf.keras.layers.Dense(units=10, activation='softmax') # Using 1 layer of Dense layer with 10 neurons and softmax activation\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db60698",
   "metadata": {},
   "source": [
    "Activation function tells the model about what to do. We haven't specified this activation function in the previous codes. This neural network has 3 layers - First is the flatten layer, then the Dense layer with 128 neurons and relu activation function, the last layer is the Dense layer with softmax activation function and has 10 neurons. In the last layer, the number of neurons should be equal to that of the number of classes in the target (which is 10 in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67665493-3d75-4501-b0c4-bc166a8659b5",
   "metadata": {},
   "source": [
    "An activation function introduces non-linearity into a neural network, allowing it to learn complex patterns. It decides whether a neuron should be activated based on its input. Two of the activation functions used above are -<br/>\n",
    " <ol> <li> ReLU (Rectified Linear Unit) - Common in hidden layers to prevent vanishing gradients and improve training speed. \n",
    "<ul> <li>Formula: f(x)=max(0,x) </li>\n",
    "<li>How it Works: Outputs x if positive, else 0. </li></ul> </li> <br/>\n",
    "<li> Softmax function - \n",
    "<ul> <li>Softmax converts logits into probabilities that sum to 1. Used in the final layer of multi-class classification models usually.</li></ul> </li> </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564c3d2",
   "metadata": {},
   "source": [
    "Let's check how the activation functions works in code -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "505badd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 3., 4., 2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will take an array\n",
    "inputs = np.array([-1.0, 0, -2.0, 1.0, 3.0, 4.0, 2.0])\n",
    "outputs = tf.keras.activations.relu(inputs) \n",
    "outputs.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60da577",
   "metadata": {},
   "source": [
    "Here you can see that the 0 and positive numbers passed through as it is but the negative numbers are turned to 0. This is what relu activation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f11f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00426308, 0.01158826, 0.0015683 , 0.03150015, 0.2327564 , 0.6326975 , 0.0856263 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check for softmax now. We will take same array\n",
    "inputs = np.array([[-1.0, 0, -2.0, 1.0, 3.0, 4.0, 2.0]])\n",
    "\n",
    "# Softmax functions requires array to be 2D and of tensor type. So you need to convert the array into a tensor\n",
    "inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "#print(inputs)\n",
    "\n",
    "outputs = tf.keras.activations.softmax(inputs) \n",
    "outputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ad2adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.9999999999999999>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax gives probability scores attached to each number and the higher number will get higher score\n",
    "\n",
    "total = tf.reduce_sum(outputs)  # Output is a tensor and this is to get the sum of all the values in the tensor\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e902b08",
   "metadata": {},
   "source": [
    "So the total is close to 1, which is correct as total probability cannot be greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "252162ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the highest number gets the highest probability, that will be our prediction\n",
    "\n",
    "prediction_value = np.argmax(outputs)  # Getting the index of the highest value in the output\n",
    "prediction_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db17f560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the result is \n",
    "\n",
    "inputs[0][prediction_value].numpy()  # Result at the position of 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db11a60",
   "metadata": {},
   "source": [
    "Okay, now we know how the activation functions are working. Let's compile the model -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44adf481",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572da4d",
   "metadata": {},
   "source": [
    "We could have written the above as - <br/>\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy']) <br/>\n",
    "Now accuracy is used as a metric, which will show the accuracy value at each epoch besides the loss value. All images will go through the network, that is through all the neurons, for 10 times as specified in epoch. The final loss and accuracy can be seen at the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d47e1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7796 - loss: 0.6387\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8642 - loss: 0.3824\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8787 - loss: 0.3331\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8847 - loss: 0.3149\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8951 - loss: 0.2901\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8967 - loss: 0.2780\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8992 - loss: 0.2669\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9041 - loss: 0.2593\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9081 - loss: 0.2464\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9108 - loss: 0.2385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x212de467310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the model training part\n",
    "model.fit(training_images, training_labels, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e793836",
   "metadata": {},
   "source": [
    "Our model is 91.21% accurate in guessing the images as per the labels. Now how will it perform on unseen data? In order to measure that, we have evaluate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7af547ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8834 - loss: 0.3312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.334062784910202, 0.8824999928474426]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing_images, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79da2e",
   "metadata": {},
   "source": [
    "The accuracy here is around 88%, which is less than the training accuracy. Let's try predicting an image from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61775d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb25ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.4305269e-04 6.3614880e-07 2.6898239e-05 9.9857926e-01 2.4119647e-06 4.6222468e-08 6.4739137e-04 1.2863491e-10 3.2082983e-07 4.2897124e-08]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa249b7",
   "metadata": {},
   "source": [
    "Prediction shows the probability that the image at index 100 is having for each of the 10 classes. We will get the argument for the maximum value out of it -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f848795e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[100])  # It belongs to class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4ae882a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels[100]  # Thus we can see our prediction is right here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439ec2b",
   "metadata": {},
   "source": [
    "<h3>So what you did here? </h3><br/>\n",
    "<ol>\n",
    "    <li> Used the Fashion Mnist dataset </li>\n",
    "    <li> Looked into the basic data details </li>\n",
    "    <li> Normalized the data </li>\n",
    "    <li> You created the model architecture with - <ol>\n",
    "        <li>Input layer for the incoming data</li>\n",
    "        <li>a Flatten layer</li>\n",
    "        <li>2 dense neural network layers with 128 neurons and 10 neurons alongwith activation functions </li></ol></li>\n",
    "    <li> Compiled the model with loss and optimizer </li>\n",
    "    <li> Fitted the model with training data </li>\n",
    "    <li> Evaluated the model with test data </li>\n",
    "    <li> Predicted on the trained model </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c47bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d3338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e34af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f76b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6859069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
