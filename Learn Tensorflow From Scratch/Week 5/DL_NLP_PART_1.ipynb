{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f44b65",
   "metadata": {},
   "source": [
    "In NLP, the first step is building a vocabulary from the text corpus, followed by converting text into numerical features for training neural networks. TensorFlow and Keras simplify this with APIs like the TextVectorization preprocessing layer. Using its adapt() method, it processes a list of sentences, assigning each unique word an integer ID.\n",
    "\n",
    "The TextVectorization layer tokenizes text and converts words into numerical representations. Key hyperparameters include max_tokens (vocabulary size), output_mode (int, binary, TF-IDF, or n-grams), output_sequence_length (fixed-length sequences), and standardize (text preprocessing options). It supports the adapt() method for learning vocabulary from data and is used in NLP pipelines before embedding layers or models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b305311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sentences = ['i love my dog', 'I, love my cat']\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary(include_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7031e",
   "metadata": {},
   "source": [
    "- tf.keras.layers.TextVectorization() : Initializes a TextVectorization layer, which will process text and generate a vocabulary. Default settings:\n",
    "    - max_tokens=None (unlimited vocabulary)\n",
    "    - output_mode='int' (integer tokenization)\n",
    "    - standardize='lower_and_strip_punctuation' (lowercases text and removes punctuation)\n",
    "    \n",
    "<br/>\n",
    "\n",
    "- vectorize_layer.adapt(sentences) - Learns the vocabulary from the given sentences. The text is lowercased, and punctuation is removed (default standardization)\n",
    "<br/>\n",
    "\n",
    "- vocabulary = vectorize_layer.get_vocabulary(include_special_tokens=False). Retrieves the learned vocabulary as a list of words, excluding special tokens (e.g., [UNK] for unknown words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26e4756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'love', 'i', 'dog', 'cat']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary  # Unique words are stored in order of appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ab9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add another sentence in it\n",
    "\n",
    "sentences = ['i love my dog', 'I, love my cat', 'You love my dog!']\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary(include_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b14312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  my\n",
      "1  -  love\n",
      "2  -  i\n",
      "3  -  dog\n",
      "4  -  you\n",
      "5  -  cat\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(vocabulary):  # More frequent words have lower indices when using TextVectorization\n",
    "    print(index, ' - ', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c60c236f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'love', 'i', 'dog', 'you', 'cat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i love my dog', 'I, love my cat', 'You love my dog!']\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary(include_special_tokens=False)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8126f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'my', 'love', 'i', 'dog', 'you', 'cat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Including special tokens for handling unknown words or for padding\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "vocabulary  # 0 is for padding and 1 is used for out of vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d86ef4",
   "metadata": {},
   "source": [
    "Text data has to be converted into numeric sequence and needs to be of uniform size before feeling into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88346207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  \n",
      "1  -  [UNK]\n",
      "2  -  my\n",
      "3  -  love\n",
      "4  -  dog\n",
      "5  -  you\n",
      "6  -  i\n",
      "7  -  think\n",
      "8  -  is\n",
      "9  -  do\n",
      "10  -  cat\n",
      "11  -  amazing\n"
     ]
    }
   ],
   "source": [
    "sentences = ['i love my dog', 'I, love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "for index, words in enumerate(vocabulary):\n",
    "    print(index, ' - ', words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2a445",
   "metadata": {},
   "source": [
    "Now we can use this result to convert sentences into integer sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff259350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([6, 3, 2, 4], dtype=int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input = 'I love my dog'\n",
    "sequence = vectorize_layer(sample_input)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376fb28",
   "metadata": {},
   "source": [
    "String is passed to the layer that has learned the vocabulary and it will output integer sequence as a tf.Tensor. Now for a given list of input sequences, this layer has to be applied to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f804e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [6 3 2 4]\n",
      "I, love my cat  ---->  [ 6  3  2 10]\n",
      "You love my dog!  ---->  [5 3 2 4]\n",
      "Do you think my dog is amazing?  ---->  [ 9  5  7  2  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "sentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)  # converting list to tf.data.Dataset\n",
    "sequences = sentences_dataset.map(vectorize_layer)\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequences):\n",
    "    print(f'{sentence}  ---->  {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f2bb8",
   "metadata": {},
   "source": [
    "Integer sequences have varying lengths, making them unsuitable for direct model input. To standardize them, we apply either padding or truncation, with padding being the preferred approach to retain information. The vocabulary assigns index 0 as a special token for padding. When passing a list of string inputs to the layer, post-padding is applied, adding 0s to sequences until they match the longest sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7629c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [6 3 2 4 0 0 0]\n",
      "I, love my cat  ---->  [ 6  3  2 10  0  0  0]\n",
      "You love my dog!  ---->  [5 3 2 4 0 0 0]\n",
      "Do you think my dog is amazing?  ---->  [ 9  5  7  2  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "sequence_post = vectorize_layer(sentences)\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequence_post):\n",
    "    print(f'{sentence}  ---->  {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1867ca",
   "metadata": {},
   "source": [
    "If you want pre-padding then you can use the pad_sequences() utility to prepend a padding token to the sequences. Notice that the padding argument is set to pre. This is just for clarity. The function already has this set as the default so can opt to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "626e9784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [0 0 0 6 3 2 4]\n",
      "I, love my cat  ---->  [ 0  0  0  6  3  2 10]\n",
      "You love my dog!  ---->  [0 0 0 5 3 2 4]\n",
      "Do you think my dog is amazing?  ---->  [ 9  5  7  2  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "sequences_pre = tf.keras.utils.pad_sequences(sequences, padding='pre')\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequences_pre):\n",
    "    print(f'{sentence}  ---->  {sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c572788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [0 6 3 2 4]\n",
      "I, love my cat  ---->  [ 0  6  3  2 10]\n",
      "You love my dog!  ---->  [0 5 3 2 4]\n",
      "Do you think my dog is amazing?  ---->  [ 7  2  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "# You can set max length of padding too \n",
    "sequences_pre = tf.keras.utils.pad_sequences(sequences, padding='pre', maxlen=5) # Will keep the last 5\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequences_pre):\n",
    "    print(f'{sentence}  ---->  {sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0740f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [6 3 2 4 0]\n",
      "I, love my cat  ---->  [ 6  3  2 10  0]\n",
      "You love my dog!  ---->  [5 3 2 4 0]\n",
      "Do you think my dog is amazing?  ---->  [9 5 7 2 4]\n"
     ]
    }
   ],
   "source": [
    "# By default, the tokens will truncate from the front as seen above\n",
    "# You can truncate the tokens from the end too\n",
    "\n",
    "sequences_pre = tf.keras.utils.pad_sequences(sequences, padding='post', maxlen=5, truncating='post') \n",
    "\n",
    "for sentence, sequence in zip(sentences, sequences_pre):\n",
    "    print(f'{sentence}  ---->  {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a06855",
   "metadata": {},
   "source": [
    "Another way to prepare for prepadding is to set the TextVectorization to output ragged tensor. This means the output will not be automatically post padded -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "900eae9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[6, 3, 2, 4], [6, 3, 2, 10], [5, 3, 2, 4], [9, 5, 7, 2, 4, 8, 11]]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "vectorize_layer.adapt(sentences)\n",
    "ragged_sequences = vectorize_layer(sentences)\n",
    "ragged_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1433cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  6,  3,  2,  4],\n",
       "       [ 0,  0,  0,  6,  3,  2, 10],\n",
       "       [ 0,  0,  0,  5,  3,  2,  4],\n",
       "       [ 9,  5,  7,  2,  4,  8, 11]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_pre = tf.keras.utils.pad_sequences(ragged_sequences.numpy())\n",
    "sequences_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd027c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  3,  2,  4,  0,  0,  0],\n",
       "       [ 6,  3,  2, 10,  0,  0,  0],\n",
       "       [ 5,  3,  2,  4,  0,  0,  0],\n",
       "       [ 9,  5,  7,  2,  4,  8, 11]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_post = tf.keras.utils.pad_sequences(ragged_sequences.numpy(), padding='post')\n",
    "sequence_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda42eab",
   "metadata": {},
   "source": [
    "Now to look into the Out of vocabulary words. The layer will use the token of 1 to handle out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac47e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really love my dog  ----->  [6 1 3 2 4]\n",
      "my dog loves my manatee  ----->  [2 4 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "sentences_with_oov = ['i really love my dog', 'my dog loves my manatee']\n",
    "\n",
    "sequences_with_oov = vectorize_layer(sentences_with_oov)\n",
    "\n",
    "for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n",
    "    print(f'{sentence}  ----->  {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308bf8a8",
   "metadata": {},
   "source": [
    "### Let's conclude all at once here now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374e08d",
   "metadata": {},
   "source": [
    "#### Creating Tokens from sentences with post padding and handling OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f23c7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  tf.Tensor([ 2  4  3 11  0  0  0  0], shape=(8,), dtype=int64)\n",
      "I, love my cat  ---->  tf.Tensor([ 2  4  3 12  0  0  0  0], shape=(8,), dtype=int64)\n",
      "I have been to the place multiple times  ---->  tf.Tensor([ 2 10 13  5  7  8  9  6], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List of sentences\n",
    "sentences = ['i love my dog', 'I, love my cat', 'I have been to the place multiple times']\n",
    "\n",
    "# Initializing text vectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Adapting the layer with the sentences\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# To look at the vocabulary\n",
    "vocabulary = vectorize_layer.get_vocabulary()  # include_special_tokens=False\n",
    "\n",
    "# Vectorizing the sentences with post padding\n",
    "sequence_post = vectorize_layer(sentences)\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequence_post):\n",
    "    print(sentence, ' ----> ', sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0b2a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [ 2  4  3 11  0  0  0  0]\n",
      "I, love my cat  ---->  [ 2  4  3 12  0  0  0  0]\n",
      "I have been to the place multiple times  ---->  [ 2 10 13  5  7  8  9  6]\n"
     ]
    }
   ],
   "source": [
    "# Other way of post padding\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "vectorize_layer.adapt(sentences)\n",
    "ragged_sequences = vectorize_layer(sentences)\n",
    "sequence_post = tf.keras.utils.pad_sequences(ragged_sequences.numpy(), padding='post') #maxlen, truncating\n",
    "\n",
    "for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n",
    "    print(f'{sentence}  ----->  {sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66fe743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to travel  ----->  [2 4 5 1]\n"
     ]
    }
   ],
   "source": [
    "sentences_with_oov = ['I love to travel']\n",
    "sequences_with_oov = vectorize_layer(sentences_with_oov)\n",
    "sequences_with_oov = tf.keras.utils.pad_sequences(sequences_with_oov.numpy(), padding='post')\n",
    "\n",
    "for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n",
    "    print(f'{sentence}  ----->  {sequence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77876527",
   "metadata": {},
   "source": [
    "#### Creating Tokens from sentences with pre padding and handling OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a03eff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog  ---->  [ 0  0  0  0  2  4  3 11]\n",
      "I, love my cat  ---->  [ 0  0  0  0  2  4  3 12]\n",
      "I have been to the place multiple times  ---->  [ 2 10 13  5  7  8  9  6]\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "vectorize_layer.adapt(sentences)\n",
    "ragged_sequences = vectorize_layer(sentences)\n",
    "sequence_pre = tf.keras.utils.pad_sequences(ragged_sequences.numpy(), padding='pre') #maxlen, truncating\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequence_pre):\n",
    "    print(sentence, ' ----> ', sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11444963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to travel  ----->  [2 4 5 1]\n",
      "I love  ----->  [0 0 2 4]\n"
     ]
    }
   ],
   "source": [
    "sentences_with_oov = ['I love to travel', 'I love']\n",
    "sequences_with_oov = vectorize_layer(sentences_with_oov)\n",
    "sequences_with_oov = tf.keras.utils.pad_sequences(sequences_with_oov.numpy(), padding='pre')\n",
    "\n",
    "for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n",
    "    print(f'{sentence}  ----->  {sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f7a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
